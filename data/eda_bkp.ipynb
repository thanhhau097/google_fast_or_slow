{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import copy\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_obj_mbs(obj, cp=True):\n",
    "    return sys.getsizeof(copy.deepcopy(obj) if cp else obj) / (1<<20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"./npz_all/npz\")\n",
    "# collection = \"layout/xla\"\n",
    "collection = \"layout/nlp\"\n",
    "# ctype = \"default\"\n",
    "ctype = \"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_nodes(data, expand=1):\n",
    "    in_node_ids = data[\"node_config_ids\"]\n",
    "\n",
    "    for _ in range(expand):\n",
    "        in_edge_index = data[\"edge_index\"][np.isin(data[\"edge_index\"], in_node_ids).any(1)]\n",
    "        in_node_ids = np.unique(in_edge_index)\n",
    "\n",
    "    return in_node_ids, in_edge_index\n",
    "\n",
    "\n",
    "def prune_graph(data, expand=1):\n",
    "    # print(\"Pruning graph...\")\n",
    "    new_data = deepcopy(dict(data))\n",
    "    # print(\"Original graph has {} nodes and {} edges\".format(data[\"node_feat\"].shape[0], data[\"edge_index\"].shape[0]))\n",
    "\n",
    "    in_node_ids, in_edge_index = expand_nodes(data, expand=expand)\n",
    "\n",
    "    # assert len(set(data[\"node_config_ids\"]) - set(in_node_ids)) == 0\n",
    "\n",
    "    lookup = np.ones(data[\"node_feat\"].shape[0]) * -1\n",
    "    lookup[in_node_ids] = np.arange(in_node_ids.shape[0])\n",
    "\n",
    "    in_node_feats = data[\"node_feat\"][in_node_ids, :]\n",
    "    in_node_opcode = data[\"node_opcode\"][in_node_ids]\n",
    "    in_edge_index = lookup[in_edge_index]\n",
    "    in_node_config_ids = lookup[data[\"node_config_ids\"]]\n",
    "\n",
    "    new_data[\"node_feat\"] = in_node_feats\n",
    "    new_data[\"node_opcode\"] = in_node_opcode\n",
    "    new_data[\"edge_index\"] = in_edge_index\n",
    "    new_data[\"node_config_ids\"] = in_node_config_ids\n",
    "    print(\"New graph has {}/{} nodes and {}/{} edges\".format(data[\"node_feat\"].shape[0], new_data[\"node_feat\"].shape[0], data[\"edge_index\"].shape[0], new_data[\"edge_index\"].shape[0]))\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgraph(G, nodes):\n",
    "    H = nx.DiGraph()\n",
    "    \n",
    "    # For each node of interest, find all reachable nodes from it\n",
    "    for src in nodes:\n",
    "        reachable = nx.single_source_shortest_path_length(G, src)\n",
    "        \n",
    "        # For each node that's reachable and is also in our nodes_of_interest list, add an edge\n",
    "        for dst, _ in reachable.items():\n",
    "            if dst in nodes_of_interest and src != dst:\n",
    "                H.add_edge(src, dst)\n",
    "\n",
    "    return H\n",
    "\n",
    "def prune_graph_new(data):\n",
    "    new_data = deepcopy(dict(data))\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(data[\"edge_index\"])\n",
    "\n",
    "    nodes = expand_nodes(data[\"node_config_ids\"], expand=1)\n",
    "    H = get_subgraph(G, nodes)\n",
    "\n",
    "    in_node_ids = np.array(list(H.nodes))\n",
    "    in_edge_index = np.array(list(H.edges))\n",
    "\n",
    "    lookup = np.ones(data[\"node_feat\"].shape[0]) * -1\n",
    "    lookup[in_node_ids] = np.arange(in_node_ids.shape[0])\n",
    "\n",
    "    in_node_feats = data[\"node_feat\"][in_node_ids, :]\n",
    "    in_node_opcode = data[\"node_opcode\"][in_node_ids]\n",
    "    in_edge_index = lookup[in_edge_index]\n",
    "    in_node_config_ids = lookup[data[\"node_config_ids\"]]\n",
    "\n",
    "    new_data[\"node_feat\"] = in_node_feats\n",
    "    new_data[\"node_opcode\"] = in_node_opcode\n",
    "    new_data[\"edge_index\"] = in_edge_index\n",
    "    new_data[\"node_config_ids\"] = in_node_config_ids\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dupplicated_node_configs(data):\n",
    "    reshaped_config_feat = data[\"node_config_feat\"].reshape(data[\"node_config_feat\"].shape[0], -1) + 2 # avoid zeros\n",
    "    positional_array = np.random.random(reshaped_config_feat.shape[1])  # multiply each value by its position to avoid removing permutations by accident\n",
    "    reshaped_values = (reshaped_config_feat * positional_array[None, :]).sum(1)\n",
    "    is_equal_matrix = reshaped_values[None, :] == reshaped_values[:, None] # quadratic matrix of all pairwise equalities\n",
    "    # is_equal_matrix[np.triu_indices(is_equal_matrix.shape[0], 0)] = 0 # only get diagonal to avoid remove twice\n",
    "    is_equal_matrix = np.tril(is_equal_matrix, -1) # only get diagonal to avoid remove twice\n",
    "    to_remove_ids = np.unique(np.where(is_equal_matrix)[0])\n",
    "    # print(\"Removing {} duplicated node configs out of {}\".format(to_remove_ids.shape[0], data[\"node_config_feat\"].shape[0]))\n",
    "    data[\"config_runtime\"] = np.delete(data[\"config_runtime\"], to_remove_ids)\n",
    "    data[\"node_config_feat\"] = np.delete(data[\"node_config_feat\"], to_remove_ids, axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_rows(data):\n",
    "    matrix = data[\"node_config_feat\"].reshape(data[\"node_config_feat\"].shape[0], -1).astype(np.int32)\n",
    "\n",
    "    # Get unique rows and inverse index\n",
    "    _, unique_idx, inverse = np.unique(matrix, axis=0, return_index=True, return_inverse=True)\n",
    "    \n",
    "    # Create a dictionary of duplicates\n",
    "    duplicates = {}\n",
    "    for i, inv in enumerate(inverse):\n",
    "        if list(np.where(inverse == inv)[0]) != [i]:\n",
    "            duplicates.setdefault(unique_idx[inv], []).append(i)\n",
    "    \n",
    "    # Filter out entries with only one index (i.e., unique rows)\n",
    "    dup_config_dct = {k: np.array(v) for k, v in duplicates.items() if len(v) > 1}\n",
    "\n",
    "    all_dup_idx = [v[v != k] for k, v in dup_config_dct.items()]\n",
    "    all_dup_idx = np.concatenate(all_dup_idx) if len(all_dup_idx) else []\n",
    "\n",
    "    return dup_config_dct, all_dup_idx\n",
    "\n",
    "\n",
    "def dedup_configs(data):\n",
    "    dup_config_dct, all_dup_idx = find_duplicate_rows(data)\n",
    "\n",
    "    for org_idx, idx_list in dup_config_dct.items():\n",
    "        data[\"config_runtime\"][org_idx] = round(np.mean(data[\"config_runtime\"][idx_list]))\n",
    "\n",
    "    if len(all_dup_idx):\n",
    "        data[\"config_runtime\"] = np.delete(data[\"config_runtime\"], all_dup_idx)\n",
    "        data[\"node_config_feat\"] = np.delete(data[\"node_config_feat\"], all_dup_idx, axis=0)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test_dedup_configs(data):\n",
    "    res = remove_dupplicated_node_configs(copy.deepcopy(data))[\"node_config_feat\"].shape == dedup_configs(copy.deepcopy(data))[\"node_config_feat\"].shape\n",
    "    assert res\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_int(vec: np.ndarray) -> np.ndarray:\n",
    "    # Powers of 7: [1, 7, 49, 343, 2401, 16807]\n",
    "    powers_of_7 = np.array([7**i for i in range(6)])\n",
    "    return np.dot(vec, powers_of_7).astype(np.int32)\n",
    "\n",
    "\n",
    "def int_to_vec(integers: np.ndarray) -> np.ndarray:\n",
    "    # Create an empty array of shape (N, 6) to store the results\n",
    "    vectors = np.empty((len(integers), 6), dtype=np.int64)\n",
    "\n",
    "    # Divide by powers of 7 and take the remainder to find each digit\n",
    "    for i in range(6):\n",
    "        vectors[:, i] = integers % 7\n",
    "        integers //= 7\n",
    "\n",
    "    return vectors.astype(np.int32)\n",
    "\n",
    "\n",
    "def compress_configs(node_configs):\n",
    "    vecs = node_configs.reshape(-1, 6).astype(np.int32) + 1\n",
    "    ints = vec_to_int(vecs)\n",
    "    ints = ints.reshape(node_configs.shape[0], node_configs.shape[1], 3)\n",
    "    return ints\n",
    "\n",
    "\n",
    "def decompress_configs(node_configs):\n",
    "    ints = node_configs.astype(np.int32).reshape(-1)\n",
    "    vecs = int_to_vec(ints)\n",
    "    vecs = vecs.reshape(node_configs.shape[0], -1, 18) - 1\n",
    "    return vecs\n",
    "\n",
    "\n",
    "def test_compression(data, db=False):\n",
    "    org = data[\"node_config_feat\"].astype(np.int32)\n",
    "    comp = compress_configs(data[\"node_config_feat\"])\n",
    "    decomp = decompress_configs(comp)\n",
    "\n",
    "    if db:\n",
    "        print(org.shape, comp.shape, decomp.shape)\n",
    "        print(org[0, :2], comp[0, :2], decomp[0, :2], sep=\"\\n\")\n",
    "        print(get_obj_mbs(org), get_obj_mbs(comp), get_obj_mbs(decomp))\n",
    "    \n",
    "    res = (org == decomp).all()\n",
    "\n",
    "    assert res\n",
    "    assert round(get_obj_mbs(org) / get_obj_mbs(comp)) == 6\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_dir = root / f\"{collection}_pruned_new\" / ctype\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    print(\"Loading {} data...\".format(split))\n",
    "    split_src_dir = root / collection / ctype / split\n",
    "    split_dst_dir = dst_dir / split\n",
    "    split_dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for npz_path in tqdm(list(split_src_dir.glob(\"*.npz\"))):\n",
    "        out_p = split_dst_dir / npz_path.name\n",
    "\n",
    "        if out_p.exists():\n",
    "            continue\n",
    "\n",
    "        data = dict(np.load(str(npz_path), allow_pickle=True))\n",
    "\n",
    "        # data = prune_graph(data, expand=3)\n",
    "        data = prune_graph_new(data, expand=1)\n",
    "        \n",
    "        # if split == \"train\":\n",
    "        #       data = dedup_configs(data)\n",
    "        #     # assert test_dedup_configs(data)\n",
    "\n",
    "        if split != \"test\":\n",
    "            data = dedup_configs(data)\n",
    "\n",
    "        # if split == \"valid\":\n",
    "        #     if len(data[\"config_runtime\"]) <= 10000:\n",
    "        #         sel_idx = np.arange(data[\"config_runtime\"].shape[0])\n",
    "        #     else:\n",
    "        #         best_idx = np.argsort(data[\"config_runtime\"])[:5000]\n",
    "        #         sel_idx = np.random.choice(np.arange(data[\"config_runtime\"].shape[0]), 5000, replace=False)\n",
    "        #         sel_idx = np.unique(np.concatenate([best_idx, sel_idx]))\n",
    "\n",
    "        #     print(data[\"config_runtime\"].shape[0], sel_idx.shape[0])\n",
    "\n",
    "        #     data[\"node_config_feat\"] = data[\"node_config_feat\"][sel_idx]\n",
    "        #     data[\"config_runtime\"] = data[\"config_runtime\"][sel_idx]\n",
    "\n",
    "        # assert test_compression(data)\n",
    "        data[\"node_config_feat\"] = compress_configs(data[\"node_config_feat\"])\n",
    "\n",
    "        # np.savez(split_dst_dir / npz_path.name, **data)\n",
    "        np.savez_compressed(split_dst_dir / npz_path.name, **data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def contains_cycle(edge_matrix):\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edge_matrix)\n",
    "    try:\n",
    "        nx.find_cycle(G)\n",
    "        return True\n",
    "    except nx.NetworkXNoCycle:\n",
    "        return False\n",
    "\n",
    "def plot_graph(edge_matrix, node_names):\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edge_matrix)\n",
    "    \n",
    "    # Using a different layout and adjusting its parameters\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "    \n",
    "    plt.figure(figsize=(12,12))  # Increase as needed\n",
    "    \n",
    "    # Reducing node size and edge width\n",
    "    nx.draw(G, pos, with_labels=False, node_size=10, width=0.1)\n",
    "    \n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_graph(data[\"edge_index\"], data[\"node_opcode\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_tpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
